
train_transform = T.Compose([
    T.Resize((256, 256)),
    T.RandomResizedCrop(224, scale=(0.8, 1.0)), 
    T.RandomHorizontalFlip(p=0.5),
    T.RandomRotation(15),  
    T.RandomAffine(
        degrees=0,
        translate=(0.1, 0.1),
        scale=(0.9, 1.1)
    ),
    T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
    T.RandomGrayscale(p=0.1),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    T.RandomErasing(p=0.2, scale=(0.02, 0.2))
])

val_transform = T.Compose([
    T.Resize((256, 256)),
    T.CenterCrop(224),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


class_attributes = torch.tensor(attributes, dtype=torch.float32).to(device) # (200, 312)
# class_attributes = (class_attributes - class_attributes.mean(0)) / class_attributes.std(0)

scale=30
model = ResNet18AttrMultiTask(class_attributes, scale=scale).to(device)

criterion = nn.CrossEntropyLoss()

learning_rate = 1e-4
#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4) # STRICTLY BETTER

batch_size = 64
alpha=0.3

# scheduler: reduce LR when val_loss plateaus
# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

# CAREFUL TO NOT OVERWRITE
model_name = "base_model.pt"
model_dir = "cnn_models"
model_path = os.path.join(model_dir, model_name)