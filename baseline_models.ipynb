{"cells":[{"cell_type":"code","execution_count":4,"id":"9d875c5a","metadata":{"id":"9d875c5a","executionInfo":{"status":"ok","timestamp":1765289872628,"user_tz":-60,"elapsed":2,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dv8I1RINXCta","executionInfo":{"status":"ok","timestamp":1765289899496,"user_tz":-60,"elapsed":26108,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}},"outputId":"d5e82308-ec04-4610-b415-d07480c1206a"},"id":"dv8I1RINXCta","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"45a801a3","metadata":{"id":"45a801a3"},"source":["Let's load the different bird species from the `class_names.npy` file and then the attributes from `attributes.npy` which has for every class 312 features that are explained by the file `attributes.txt`."]},{"cell_type":"code","source":["glob_path = \"drive/MyDrive/\""],"metadata":{"id":"F2c9NNacXbHd","executionInfo":{"status":"ok","timestamp":1765289902219,"user_tz":-60,"elapsed":4,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}}},"id":"F2c9NNacXbHd","execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"id":"62bd6526","metadata":{"id":"62bd6526","executionInfo":{"status":"ok","timestamp":1765289904643,"user_tz":-60,"elapsed":1972,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}}},"outputs":[],"source":["bird_classes = np.load(glob_path + \"data/class_names.npy\", allow_pickle=True).item()"]},{"cell_type":"code","execution_count":8,"id":"02b155d6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"02b155d6","executionInfo":{"status":"ok","timestamp":1765289907869,"user_tz":-60,"elapsed":689,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}},"outputId":"617ca381-d06e-4c38-f5f3-85c99c94e01a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(200, 312)"]},"metadata":{},"execution_count":8}],"source":["attributes = np.load(glob_path + 'data/attributes.npy')\n","attributes.shape"]},{"cell_type":"code","execution_count":9,"id":"ff7e3716","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ff7e3716","executionInfo":{"status":"ok","timestamp":1765289909808,"user_tz":-60,"elapsed":878,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}},"outputId":"eaf98789-b54e-475a-ce5b-71c8308dea80"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1 has_bill_shape::curved_(up_or_down)',\n"," '2 has_bill_shape::dagger',\n"," '3 has_bill_shape::hooked',\n"," '4 has_bill_shape::needle',\n"," '5 has_bill_shape::hooked_seabird']"]},"metadata":{},"execution_count":9}],"source":["with open(glob_path + \"data/attributes.txt\", \"r\") as f:\n","    attribute_names = [line.strip() for line in f.readlines()]\n","\n","attribute_names[:5]"]},{"cell_type":"markdown","id":"51b21b06","metadata":{"id":"51b21b06"},"source":["Unify the attributes files to map for every bird species they're features"]},{"cell_type":"code","execution_count":10,"id":"0dc5c257","metadata":{"id":"0dc5c257","executionInfo":{"status":"ok","timestamp":1765289911824,"user_tz":-60,"elapsed":14,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}}},"outputs":[],"source":["class_attributes = {}\n","\n","for class_id in range(attributes.shape[0]):\n","    class_attributes[class_id + 1] = {\n","        attribute_names[i]: attributes[class_id, i] for i in range(len(attribute_names))\n","    }"]},{"cell_type":"markdown","id":"6cd13656","metadata":{"id":"6cd13656"},"source":["Create a data frame `birds_df` with the class_id and the 312 attrbiutes of each bird class. Then merge it with the class name of each bird."]},{"cell_type":"code","execution_count":11,"id":"6e0e2ab8","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":290},"id":"6e0e2ab8","executionInfo":{"status":"ok","timestamp":1765289913917,"user_tz":-60,"elapsed":50,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}},"outputId":"131063ea-9208-4e37-a449-7c7cc7ca41cf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   class_id  1 has_bill_shape::curved_(up_or_down)  2 has_bill_shape::dagger  \\\n","0         1                               0.010638                  0.010638   \n","1         2                               0.000000                  0.011332   \n","2         3                               0.000000                  0.000000   \n","3         4                               0.000000                  0.000000   \n","4         5                               0.000000                  0.035088   \n","\n","   3 has_bill_shape::hooked  4 has_bill_shape::needle  \\\n","0                  0.007092                  0.003546   \n","1                  0.009444                  0.000000   \n","2                  0.007425                  0.000000   \n","3                  0.003861                  0.000000   \n","4                  0.000000                  0.000000   \n","\n","   5 has_bill_shape::hooked_seabird  6 has_bill_shape::spatulate  \\\n","0                          0.138299                     0.065603   \n","1                          0.202095                     0.041552   \n","2                          0.002475                     0.000000   \n","3                          0.003861                     0.013514   \n","4                          0.000000                     0.000000   \n","\n","   7 has_bill_shape::all-purpose  8 has_bill_shape::cone  \\\n","0                       0.000000                0.005319   \n","1                       0.015110                0.005666   \n","2                       0.000000                0.074247   \n","3                       0.005792                0.073360   \n","4                       0.102458                0.070177   \n","\n","   9 has_bill_shape::specialized  ...  303 has_crown_color::pink  \\\n","0                       0.000000  ...                   0.000000   \n","1                       0.000000  ...                   0.006291   \n","2                       0.146020  ...                   0.000000   \n","3                       0.138998  ...                   0.004885   \n","4                       0.000000  ...                   0.000000   \n","\n","   304 has_crown_color::orange  305 has_crown_color::black  \\\n","0                     0.005439                    0.005439   \n","1                     0.000000                    0.111144   \n","2                     0.000000                    0.190411   \n","3                     0.000000                    0.190531   \n","4                     0.000000                    0.204036   \n","\n","   306 has_crown_color::white  307 has_crown_color::red  \\\n","0                    0.228446                  0.000000   \n","1                    0.008388                  0.000000   \n","2                    0.012555                  0.000000   \n","3                    0.000000                  0.000000   \n","4                    0.002458                  0.002458   \n","\n","   308 has_crown_color::buff  309 has_wing_pattern::solid  \\\n","0                   0.000000                     0.186020   \n","1                   0.046135                     0.202572   \n","2                   0.010462                     0.203609   \n","3                   0.000000                     0.152750   \n","4                   0.000000                     0.031640   \n","\n","   310 has_wing_pattern::spotted  311 has_wing_pattern::striped  \\\n","0                       0.009186                       0.025262   \n","1                       0.002665                       0.021323   \n","2                       0.000000                       0.008853   \n","3                       0.006840                       0.036478   \n","4                       0.002751                       0.015132   \n","\n","   312 has_wing_pattern::multi-colored  \n","0                             0.020669  \n","1                             0.058639  \n","2                             0.017705  \n","3                             0.043317  \n","4                             0.158200  \n","\n","[5 rows x 313 columns]"],"text/html":["\n","  <div id=\"df-d29be3a8-5440-4599-8b30-63b9bea7e2cd\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class_id</th>\n","      <th>1 has_bill_shape::curved_(up_or_down)</th>\n","      <th>2 has_bill_shape::dagger</th>\n","      <th>3 has_bill_shape::hooked</th>\n","      <th>4 has_bill_shape::needle</th>\n","      <th>5 has_bill_shape::hooked_seabird</th>\n","      <th>6 has_bill_shape::spatulate</th>\n","      <th>7 has_bill_shape::all-purpose</th>\n","      <th>8 has_bill_shape::cone</th>\n","      <th>9 has_bill_shape::specialized</th>\n","      <th>...</th>\n","      <th>303 has_crown_color::pink</th>\n","      <th>304 has_crown_color::orange</th>\n","      <th>305 has_crown_color::black</th>\n","      <th>306 has_crown_color::white</th>\n","      <th>307 has_crown_color::red</th>\n","      <th>308 has_crown_color::buff</th>\n","      <th>309 has_wing_pattern::solid</th>\n","      <th>310 has_wing_pattern::spotted</th>\n","      <th>311 has_wing_pattern::striped</th>\n","      <th>312 has_wing_pattern::multi-colored</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.010638</td>\n","      <td>0.010638</td>\n","      <td>0.007092</td>\n","      <td>0.003546</td>\n","      <td>0.138299</td>\n","      <td>0.065603</td>\n","      <td>0.000000</td>\n","      <td>0.005319</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.005439</td>\n","      <td>0.005439</td>\n","      <td>0.228446</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.186020</td>\n","      <td>0.009186</td>\n","      <td>0.025262</td>\n","      <td>0.020669</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0.000000</td>\n","      <td>0.011332</td>\n","      <td>0.009444</td>\n","      <td>0.000000</td>\n","      <td>0.202095</td>\n","      <td>0.041552</td>\n","      <td>0.015110</td>\n","      <td>0.005666</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.006291</td>\n","      <td>0.000000</td>\n","      <td>0.111144</td>\n","      <td>0.008388</td>\n","      <td>0.000000</td>\n","      <td>0.046135</td>\n","      <td>0.202572</td>\n","      <td>0.002665</td>\n","      <td>0.021323</td>\n","      <td>0.058639</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.007425</td>\n","      <td>0.000000</td>\n","      <td>0.002475</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.074247</td>\n","      <td>0.146020</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.190411</td>\n","      <td>0.012555</td>\n","      <td>0.000000</td>\n","      <td>0.010462</td>\n","      <td>0.203609</td>\n","      <td>0.000000</td>\n","      <td>0.008853</td>\n","      <td>0.017705</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.003861</td>\n","      <td>0.000000</td>\n","      <td>0.003861</td>\n","      <td>0.013514</td>\n","      <td>0.005792</td>\n","      <td>0.073360</td>\n","      <td>0.138998</td>\n","      <td>...</td>\n","      <td>0.004885</td>\n","      <td>0.000000</td>\n","      <td>0.190531</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.152750</td>\n","      <td>0.006840</td>\n","      <td>0.036478</td>\n","      <td>0.043317</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0.000000</td>\n","      <td>0.035088</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.102458</td>\n","      <td>0.070177</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.204036</td>\n","      <td>0.002458</td>\n","      <td>0.002458</td>\n","      <td>0.000000</td>\n","      <td>0.031640</td>\n","      <td>0.002751</td>\n","      <td>0.015132</td>\n","      <td>0.158200</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 313 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d29be3a8-5440-4599-8b30-63b9bea7e2cd')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d29be3a8-5440-4599-8b30-63b9bea7e2cd button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d29be3a8-5440-4599-8b30-63b9bea7e2cd');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-27c179a4-86a5-4da8-aa9c-b6c6676ed92a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-27c179a4-86a5-4da8-aa9c-b6c6676ed92a')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-27c179a4-86a5-4da8-aa9c-b6c6676ed92a button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"birds_df"}},"metadata":{},"execution_count":11}],"source":["birds_df = pd.DataFrame.from_dict(class_attributes, orient=\"index\")\n","birds_df.index.name = \"class_id\"\n","birds_df.reset_index(inplace=True)\n","birds_df.head()"]},{"cell_type":"code","execution_count":12,"id":"5cf3875e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"5cf3875e","executionInfo":{"status":"ok","timestamp":1765289915551,"user_tz":-60,"elapsed":36,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}},"outputId":"7938bdd0-cc4a-484c-88fa-53cd53827d92"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                        class  id\n","0  001.Black_footed_Albatross   1\n","1        002.Laysan_Albatross   2\n","2         003.Sooty_Albatross   3\n","3       004.Groove_billed_Ani   4\n","4          005.Crested_Auklet   5"],"text/html":["\n","  <div id=\"df-e15343b5-1aee-457e-bd3c-7c17f2aa3501\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>001.Black_footed_Albatross</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>002.Laysan_Albatross</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>003.Sooty_Albatross</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>004.Groove_billed_Ani</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>005.Crested_Auklet</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e15343b5-1aee-457e-bd3c-7c17f2aa3501')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e15343b5-1aee-457e-bd3c-7c17f2aa3501 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e15343b5-1aee-457e-bd3c-7c17f2aa3501');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-93fb2797-2ae3-405a-8969-cd7bfd4be0aa\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-93fb2797-2ae3-405a-8969-cd7bfd4be0aa')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-93fb2797-2ae3-405a-8969-cd7bfd4be0aa button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"classes","summary":"{\n  \"name\": \"classes\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"096.Hooded_Oriole\",\n          \"016.Painted_Bunting\",\n          \"031.Black_billed_Cuckoo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 1,\n        \"max\": 200,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          96,\n          16,\n          31\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":12}],"source":["classes = pd.DataFrame.from_dict(bird_classes, orient=\"index\").reset_index()\n","classes.columns = [\"class\", \"id\"]\n","classes.head()"]},{"cell_type":"code","execution_count":13,"id":"86f1c339","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":290},"id":"86f1c339","executionInfo":{"status":"ok","timestamp":1765289917259,"user_tz":-60,"elapsed":46,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}},"outputId":"7f1542f6-84d7-4959-9fce-07074d62f048"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   class_id                       class  \\\n","0         1  001.Black_footed_Albatross   \n","1         2        002.Laysan_Albatross   \n","2         3         003.Sooty_Albatross   \n","3         4       004.Groove_billed_Ani   \n","4         5          005.Crested_Auklet   \n","\n","   1 has_bill_shape::curved_(up_or_down)  2 has_bill_shape::dagger  \\\n","0                               0.010638                  0.010638   \n","1                               0.000000                  0.011332   \n","2                               0.000000                  0.000000   \n","3                               0.000000                  0.000000   \n","4                               0.000000                  0.035088   \n","\n","   3 has_bill_shape::hooked  4 has_bill_shape::needle  \\\n","0                  0.007092                  0.003546   \n","1                  0.009444                  0.000000   \n","2                  0.007425                  0.000000   \n","3                  0.003861                  0.000000   \n","4                  0.000000                  0.000000   \n","\n","   5 has_bill_shape::hooked_seabird  6 has_bill_shape::spatulate  \\\n","0                          0.138299                     0.065603   \n","1                          0.202095                     0.041552   \n","2                          0.002475                     0.000000   \n","3                          0.003861                     0.013514   \n","4                          0.000000                     0.000000   \n","\n","   7 has_bill_shape::all-purpose  8 has_bill_shape::cone  ...  \\\n","0                       0.000000                0.005319  ...   \n","1                       0.015110                0.005666  ...   \n","2                       0.000000                0.074247  ...   \n","3                       0.005792                0.073360  ...   \n","4                       0.102458                0.070177  ...   \n","\n","   303 has_crown_color::pink  304 has_crown_color::orange  \\\n","0                   0.000000                     0.005439   \n","1                   0.006291                     0.000000   \n","2                   0.000000                     0.000000   \n","3                   0.004885                     0.000000   \n","4                   0.000000                     0.000000   \n","\n","   305 has_crown_color::black  306 has_crown_color::white  \\\n","0                    0.005439                    0.228446   \n","1                    0.111144                    0.008388   \n","2                    0.190411                    0.012555   \n","3                    0.190531                    0.000000   \n","4                    0.204036                    0.002458   \n","\n","   307 has_crown_color::red  308 has_crown_color::buff  \\\n","0                  0.000000                   0.000000   \n","1                  0.000000                   0.046135   \n","2                  0.000000                   0.010462   \n","3                  0.000000                   0.000000   \n","4                  0.002458                   0.000000   \n","\n","   309 has_wing_pattern::solid  310 has_wing_pattern::spotted  \\\n","0                     0.186020                       0.009186   \n","1                     0.202572                       0.002665   \n","2                     0.203609                       0.000000   \n","3                     0.152750                       0.006840   \n","4                     0.031640                       0.002751   \n","\n","   311 has_wing_pattern::striped  312 has_wing_pattern::multi-colored  \n","0                       0.025262                             0.020669  \n","1                       0.021323                             0.058639  \n","2                       0.008853                             0.017705  \n","3                       0.036478                             0.043317  \n","4                       0.015132                             0.158200  \n","\n","[5 rows x 314 columns]"],"text/html":["\n","  <div id=\"df-c5ec78eb-9dab-4a61-a094-a4edfbc543a5\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class_id</th>\n","      <th>class</th>\n","      <th>1 has_bill_shape::curved_(up_or_down)</th>\n","      <th>2 has_bill_shape::dagger</th>\n","      <th>3 has_bill_shape::hooked</th>\n","      <th>4 has_bill_shape::needle</th>\n","      <th>5 has_bill_shape::hooked_seabird</th>\n","      <th>6 has_bill_shape::spatulate</th>\n","      <th>7 has_bill_shape::all-purpose</th>\n","      <th>8 has_bill_shape::cone</th>\n","      <th>...</th>\n","      <th>303 has_crown_color::pink</th>\n","      <th>304 has_crown_color::orange</th>\n","      <th>305 has_crown_color::black</th>\n","      <th>306 has_crown_color::white</th>\n","      <th>307 has_crown_color::red</th>\n","      <th>308 has_crown_color::buff</th>\n","      <th>309 has_wing_pattern::solid</th>\n","      <th>310 has_wing_pattern::spotted</th>\n","      <th>311 has_wing_pattern::striped</th>\n","      <th>312 has_wing_pattern::multi-colored</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>001.Black_footed_Albatross</td>\n","      <td>0.010638</td>\n","      <td>0.010638</td>\n","      <td>0.007092</td>\n","      <td>0.003546</td>\n","      <td>0.138299</td>\n","      <td>0.065603</td>\n","      <td>0.000000</td>\n","      <td>0.005319</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.005439</td>\n","      <td>0.005439</td>\n","      <td>0.228446</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.186020</td>\n","      <td>0.009186</td>\n","      <td>0.025262</td>\n","      <td>0.020669</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>002.Laysan_Albatross</td>\n","      <td>0.000000</td>\n","      <td>0.011332</td>\n","      <td>0.009444</td>\n","      <td>0.000000</td>\n","      <td>0.202095</td>\n","      <td>0.041552</td>\n","      <td>0.015110</td>\n","      <td>0.005666</td>\n","      <td>...</td>\n","      <td>0.006291</td>\n","      <td>0.000000</td>\n","      <td>0.111144</td>\n","      <td>0.008388</td>\n","      <td>0.000000</td>\n","      <td>0.046135</td>\n","      <td>0.202572</td>\n","      <td>0.002665</td>\n","      <td>0.021323</td>\n","      <td>0.058639</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>003.Sooty_Albatross</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.007425</td>\n","      <td>0.000000</td>\n","      <td>0.002475</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.074247</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.190411</td>\n","      <td>0.012555</td>\n","      <td>0.000000</td>\n","      <td>0.010462</td>\n","      <td>0.203609</td>\n","      <td>0.000000</td>\n","      <td>0.008853</td>\n","      <td>0.017705</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>004.Groove_billed_Ani</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.003861</td>\n","      <td>0.000000</td>\n","      <td>0.003861</td>\n","      <td>0.013514</td>\n","      <td>0.005792</td>\n","      <td>0.073360</td>\n","      <td>...</td>\n","      <td>0.004885</td>\n","      <td>0.000000</td>\n","      <td>0.190531</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.152750</td>\n","      <td>0.006840</td>\n","      <td>0.036478</td>\n","      <td>0.043317</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>005.Crested_Auklet</td>\n","      <td>0.000000</td>\n","      <td>0.035088</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.102458</td>\n","      <td>0.070177</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.204036</td>\n","      <td>0.002458</td>\n","      <td>0.002458</td>\n","      <td>0.000000</td>\n","      <td>0.031640</td>\n","      <td>0.002751</td>\n","      <td>0.015132</td>\n","      <td>0.158200</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 314 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5ec78eb-9dab-4a61-a094-a4edfbc543a5')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c5ec78eb-9dab-4a61-a094-a4edfbc543a5 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c5ec78eb-9dab-4a61-a094-a4edfbc543a5');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-81258ca0-0229-456f-a0e3-38eb293b4f69\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-81258ca0-0229-456f-a0e3-38eb293b4f69')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-81258ca0-0229-456f-a0e3-38eb293b4f69 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"birds_df"}},"metadata":{},"execution_count":13}],"source":["birds_df = birds_df.merge(classes, left_on=\"class_id\", right_on=\"id\")\n","birds_df = birds_df.drop(columns=[\"id\"])\n","\n","# Reorder columns to have class_id and class first\n","cols = [\"class_id\", \"class\"] + [c for c in birds_df.columns if c not in [\"class_id\", \"class\"]]\n","birds_df = birds_df[cols]\n","birds_df.head()"]},{"cell_type":"code","execution_count":14,"id":"313c468d","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"313c468d","executionInfo":{"status":"ok","timestamp":1765289920082,"user_tz":-60,"elapsed":986,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}},"outputId":"02036663-d6c7-4d7c-bf07-3746ca099fe7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                              image_path  label\n","0  drive/MyDrive/data/train_images/1.jpg      1\n","1  drive/MyDrive/data/train_images/2.jpg      1\n","2  drive/MyDrive/data/train_images/3.jpg      1\n","3  drive/MyDrive/data/train_images/4.jpg      1\n","4  drive/MyDrive/data/train_images/5.jpg      1"],"text/html":["\n","  <div id=\"df-169ec184-3621-4681-90b3-2b6bbd194b58\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>drive/MyDrive/data/train_images/1.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>drive/MyDrive/data/train_images/2.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>drive/MyDrive/data/train_images/3.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>drive/MyDrive/data/train_images/4.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>drive/MyDrive/data/train_images/5.jpg</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-169ec184-3621-4681-90b3-2b6bbd194b58')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-169ec184-3621-4681-90b3-2b6bbd194b58 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-169ec184-3621-4681-90b3-2b6bbd194b58');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-9e60f6be-546a-458c-99a2-5cc47ef268b2\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9e60f6be-546a-458c-99a2-5cc47ef268b2')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-9e60f6be-546a-458c-99a2-5cc47ef268b2 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"images_df","summary":"{\n  \"name\": \"images_df\",\n  \"rows\": 3926,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3926,\n        \"samples\": [\n          \"drive/MyDrive/data/train_images/3477.jpg\",\n          \"drive/MyDrive/data/train_images/464.jpg\",\n          \"drive/MyDrive/data/train_images/3858.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51,\n        \"min\": 1,\n        \"max\": 200,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          96,\n          16,\n          31\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":14}],"source":["images_df = pd.read_csv(glob_path + \"data/train_images.csv\")\n","images_df['image_path'] = glob_path + 'data' + images_df['image_path']\n","images_df.head()"]},{"cell_type":"markdown","id":"c838bccf","metadata":{"id":"c838bccf"},"source":["### Load training metadata and create train/validation split\n","\n","In this step, we load the `train_images.csv` file that contains the image paths and labels.  \n","Then we create a stratified train/validation split so that all 200 classes are represented proportionally in both sets.  \n","This split will be used to train the CNN on `train_images` and evaluate it on `val_images`.\n"]},{"cell_type":"code","source":[],"metadata":{"id":"AQYzl9EpZOXD","executionInfo":{"status":"ok","timestamp":1765289922582,"user_tz":-60,"elapsed":2,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}}},"id":"AQYzl9EpZOXD","execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"id":"29378958","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29378958","executionInfo":{"status":"ok","timestamp":1765289923425,"user_tz":-60,"elapsed":16,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}},"outputId":"15ae745a-e2ff-46f9-a742-d588338fed68"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3140, 786)"]},"metadata":{},"execution_count":15}],"source":["train_images, val_images = train_test_split(\n","    images_df,\n","    test_size=0.2,\n","    stratify=images_df[\"label\"],\n","    random_state=42\n",")\n","\n","len(train_images), len(val_images)"]},{"cell_type":"code","execution_count":16,"id":"03b61f6e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"03b61f6e","executionInfo":{"status":"ok","timestamp":1765289924640,"user_tz":-60,"elapsed":41,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}},"outputId":"154b75e6-bf3c-4878-ef9b-f04567ae3641"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                    image_path  label\n","1249  drive/MyDrive/data/train_images/1250.jpg     42\n","3882  drive/MyDrive/data/train_images/3883.jpg    193\n","686    drive/MyDrive/data/train_images/687.jpg     23\n","1452  drive/MyDrive/data/train_images/1453.jpg     49\n","2357  drive/MyDrive/data/train_images/2358.jpg     85"],"text/html":["\n","  <div id=\"df-90e79594-e806-45d9-98ac-c5773965100f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1249</th>\n","      <td>drive/MyDrive/data/train_images/1250.jpg</td>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>3882</th>\n","      <td>drive/MyDrive/data/train_images/3883.jpg</td>\n","      <td>193</td>\n","    </tr>\n","    <tr>\n","      <th>686</th>\n","      <td>drive/MyDrive/data/train_images/687.jpg</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>1452</th>\n","      <td>drive/MyDrive/data/train_images/1453.jpg</td>\n","      <td>49</td>\n","    </tr>\n","    <tr>\n","      <th>2357</th>\n","      <td>drive/MyDrive/data/train_images/2358.jpg</td>\n","      <td>85</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90e79594-e806-45d9-98ac-c5773965100f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-90e79594-e806-45d9-98ac-c5773965100f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-90e79594-e806-45d9-98ac-c5773965100f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-93cbb6d7-3cf9-4b6f-8d50-2a819d72d48d\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-93cbb6d7-3cf9-4b6f-8d50-2a819d72d48d')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-93cbb6d7-3cf9-4b6f-8d50-2a819d72d48d button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train_images","summary":"{\n  \"name\": \"train_images\",\n  \"rows\": 3140,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3140,\n        \"samples\": [\n          \"drive/MyDrive/data/train_images/2744.jpg\",\n          \"drive/MyDrive/data/train_images/1176.jpg\",\n          \"drive/MyDrive/data/train_images/2306.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51,\n        \"min\": 1,\n        \"max\": 200,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          141,\n          70,\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":16}],"source":["train_images.head()"]},{"cell_type":"markdown","id":"213bdfdd","metadata":{"id":"213bdfdd"},"source":["### Define Image Transformations for Training and Validation\n","\n","Before training a CNN, all images need to be preprocessed in a consistent way.  \n","Here, we define two sets of transformations:\n","\n","**Training transforms**\n","- **Resize to 224×224:** ResNet models expect fixed-size input.\n","- **Random horizontal flip:** A simple data augmentation step to help the model generalize.\n","- **Convert to tensor:** Converts the image to a PyTorch tensor with values in `[0,1]`.\n","- **Normalize with ImageNet statistics:** Since ResNet18 was pretrained on ImageNet, the same normalization must be applied for best performance.\n","\n","**Validation transforms**\n","- Same as above but **without augmentation**, to ensure a stable and deterministic evaluation.\n","\n","These transforms prepare raw images so they can be passed into the CNN during training and validation.\n"]},{"cell_type":"code","execution_count":17,"id":"3f5aed2e","metadata":{"id":"3f5aed2e","executionInfo":{"status":"ok","timestamp":1765289937889,"user_tz":-60,"elapsed":9017,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}}},"outputs":[],"source":["from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","import torchvision.transforms as T\n","\n","# image transforms (basic baseline)\n","train_transform = T.Compose([\n","    T.Resize((224, 224)),\n","    T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n","    T.RandomHorizontalFlip(p=0.5),\n","    T.RandomRotation(20),  # Increased from 15\n","    T.RandomAffine(\n","        degrees=0,\n","        translate=(0.15, 0.15),\n","        scale=(0.85, 1.15)\n","    ),\n","    T.ColorJitter(\n","        brightness=0.4,\n","        contrast=0.4,\n","        saturation=0.4,\n","        hue=0.15\n","    ),\n","    T.RandomGrayscale(p=0.1),\n","    T.ToTensor(),\n","    T.Normalize(mean=[0.485, 0.456, 0.406],\n","                std=[0.229, 0.224, 0.225]),\n","\n","])\n","\n","val_transform = T.Compose([\n","    T.Resize((224, 224)),\n","    T.ToTensor(),\n","    T.Normalize(mean=[0.485, 0.456, 0.406],\n","                std=[0.229, 0.224, 0.225]),\n","])"]},{"cell_type":"markdown","id":"46c42e39","metadata":{"id":"46c42e39"},"source":["### Create a custom PyTorch Dataset for bird images\n","\n","Here we define a `BirdsDataset` class that:\n","- Reads the image path and label from the DataFrame rows.\n","- Loads each image with PIL.\n","- Applies the appropriate transform (train or validation).\n","- Converts labels from 1–200 to 0–199 so they work with `nn.CrossEntropyLoss`.\n","\n","This Dataset will be used together with a DataLoader to efficiently feed batches to the CNN."]},{"cell_type":"code","execution_count":18,"id":"3cd4953f","metadata":{"id":"3cd4953f","executionInfo":{"status":"ok","timestamp":1765289940167,"user_tz":-60,"elapsed":5,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}}},"outputs":[],"source":["class BirdsDataset(Dataset):\n","    def __init__(self, df, attributes_array, transform=None, use_attributes=False):\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transform\n","        #self.processor = processor\n","        self.use_attributes = use_attributes\n","        self.attributes = torch.FloatTensor(attributes_array)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        img_path = row[\"image_path\"]\n","        label = int(row[\"label\"]) - 1\n","\n","        img = Image.open(img_path).convert(\"RGB\")\n","        #inputs = self.processor(images=img, return_tensors=\"pt\")\n","        #pixel_values = inputs[\"pixel_values\"].squeeze(0)\n","        pixel_values = self.transform(img)\n","        attr_vector = self.attributes[label]\n","        #if self.transform:\n","        #    img = self.transform(img)\n","\n","        return pixel_values, label, attr_vector"]},{"cell_type":"markdown","id":"019fb60b","metadata":{"id":"019fb60b"},"source":["### Wrap Datasets in DataLoaders\n","\n","Now we create `DataLoader` objects for the training and validation sets.  \n","DataLoaders handle:\n","- Shuffling (for training),\n","- Batching,\n","- Parallel loading of images (with `num_workers`).\n","\n","These will be used directly in the training and evaluation loops.\n"]},{"cell_type":"code","execution_count":27,"id":"a2b75544","metadata":{"id":"a2b75544","executionInfo":{"status":"ok","timestamp":1765290295028,"user_tz":-60,"elapsed":4,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}}},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","\n","train_dataset = BirdsDataset(train_images, transform=train_transform, attributes_array=attributes)\n","val_dataset   = BirdsDataset(val_images,   transform=val_transform, attributes_array=attributes)\n","\n","batch_size = 64\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=4,\n","    pin_memory=True\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=4,\n","    pin_memory=True\n",")"]},{"cell_type":"markdown","id":"a487fe7c","metadata":{"id":"a487fe7c"},"source":["### Define Device (GPU or CPU)"]},{"cell_type":"code","execution_count":28,"id":"2ea1448a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ea1448a","executionInfo":{"status":"ok","timestamp":1765290297195,"user_tz":-60,"elapsed":20,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}},"outputId":"dc2f8c15-e72f-427f-8c6e-b2b27a67a540"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["import torch.nn as nn\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"]},{"cell_type":"markdown","id":"f1bec69a-23e1-43f3-8ce9-f805bc126de5","metadata":{"id":"f1bec69a-23e1-43f3-8ce9-f805bc126de5"},"source":["## Define two models: Simple CNN and ResNet18"]},{"cell_type":"markdown","id":"3b607d98","metadata":{"id":"3b607d98"},"source":["### Building a simple CNN from scratch (baseline CNN)\n","\n","Before comparing with pretrained models like ResNet18, it's useful to build a classic convolutional neural network from scratch.  \n","This gives a \"true baseline\" — a model that only learns from the bird training images, without any prior ImageNet knowledge.\n","\n","The custom CNN below contains:\n","- Three convolutional blocks (Conv → BatchNorm → ReLU → MaxPool)\n","- A flatten layer\n","- Two fully-connected layers\n","- A final output layer with 200 logits (one per bird species)\n","\n","This model is lightweight, easy to understand, and suitable for verifying that the training loop and data pipeline work correctly.\n"]},{"cell_type":"code","execution_count":29,"id":"630a061d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"630a061d","executionInfo":{"status":"ok","timestamp":1765290300320,"user_tz":-60,"elapsed":111,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}},"outputId":"21865719-3791-49d9-d380-ac246e556e29"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.9.0+cu126\n","Device: cuda\n","Model device: cuda:0\n","Input device: cuda:0\n","Target device: cuda:0\n","\n","Training for 100 steps...\n","Step 20: Loss = 1.5672\n","Step 40: Loss = 0.8478\n","Step 60: Loss = 0.3420\n","Step 80: Loss = 0.1307\n","Step 100: Loss = 0.0628\n","\n","✅ Training worked! Check nvidia-smi to see GPU usage.\n"]}],"source":["print(torch.__version__)\n","\n","# Check CUDA\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device: {device}\")\n","\n","if device.type == 'cpu':\n","    print(\"❌ CUDA NOT AVAILABLE - Using CPU\")\n","    print(\"Your PyTorch installation doesn't have CUDA support!\")\n","    exit()\n","\n","# Simple model\n","model = nn.Sequential(\n","    nn.Linear(100, 50),\n","    nn.ReLU(),\n","    nn.Linear(50, 10)\n",").to(device)\n","\n","print(f\"Model device: {next(model.parameters()).device}\")\n","\n","# Generate random data\n","x = torch.randn(32, 100).to(device)\n","y = torch.randint(0, 10, (32,)).to(device)\n","\n","print(f\"Input device: {x.device}\")\n","print(f\"Target device: {y.device}\")\n","\n","# Train for a few steps\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss()\n","\n","print(\"\\nTraining for 100 steps...\")\n","for i in range(100):\n","    optimizer.zero_grad()\n","    output = model(x)\n","    loss = criterion(output, y)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (i + 1) % 20 == 0:\n","        print(f\"Step {i+1}: Loss = {loss.item():.4f}\")\n","\n","print(\"\\n✅ Training worked! Check nvidia-smi to see GPU usage.\")"]},{"cell_type":"code","execution_count":null,"id":"1c45182e","metadata":{"id":"1c45182e"},"outputs":[],"source":["\"\"\"\n","    NEGEEEEEEEEEEEEEEEEEER DIT\n","\"\"\"\n","\n","class ImprovedCNNMultiTask(nn.Module):\n","    def __init__(self, num_classes=200, num_attributes=312):\n","        super().__init__()\n","\n","        # Block 1: 3 → 64\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)  # 224 → 112\n","        )\n","\n","        # Block 2: 64 → 128\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)  # 112 → 56\n","        )\n","\n","        # Block 3: 128 → 256\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)  # 56 → 28\n","        )\n","\n","        # Block 4: 256 → 512\n","        self.conv4 = nn.Sequential(\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)  # 28 → 14\n","        )\n","\n","        # Calculate the flattened size dynamically\n","        self._to_linear = None\n","        self._get_conv_output_size()\n","\n","        # Feature extraction\n","        self.feature_fc = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(self._to_linear, 1024),\n","            nn.ReLU()\n","        )\n","\n","        # Task 1: Classification head\n","        self.class_head = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","        # Task 2: Attribute prediction head\n","        self.attr_head = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, num_attributes)\n","        )\n","\n","    def _get_conv_output_size(self):\n","        \"\"\"Calculate the output size after conv layers\"\"\"\n","        with torch.no_grad():\n","            dummy_input = torch.zeros(1, 3, 224, 224)\n","            x = self.conv1(dummy_input)\n","            x = self.conv2(x)\n","            x = self.conv3(x)\n","            x = self.conv4(x)\n","            self._to_linear = x.view(1, -1).size(1)\n","            print(f\"Flattened feature size: {self._to_linear}\")\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","\n","        x = torch.flatten(x, 1)\n","        features = self.feature_fc(x)\n","\n","        class_logits = self.class_head(features)\n","        attr_logits = self.attr_head(features)\n","\n","        return class_logits, attr_logits"]},{"cell_type":"code","execution_count":null,"id":"5ebd76e0","metadata":{"id":"5ebd76e0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"62a0436a","metadata":{"id":"62a0436a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ad2f3117","metadata":{"id":"ad2f3117"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d2447937","metadata":{"id":"d2447937"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":30,"id":"a34cc2cf","metadata":{"id":"a34cc2cf","executionInfo":{"status":"ok","timestamp":1765290305409,"user_tz":-60,"elapsed":18,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}}},"outputs":[],"source":["class ImprovedCNNMultiTask(nn.Module):\n","    def __init__(self, num_classes=200, num_attributes=312):\n","        super().__init__()\n","\n","         # Convolutional feature extractor\n","        self.features = nn.Sequential(\n","            # Block 1: 3 → 64\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),  # 224 → 112\n","\n","            # Block 2: 64 → 128\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),  # 112 → 56\n","\n","            # Block 3: 128 → 256\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),  # 56 → 28\n","\n","            # Block 4: 256 → 512\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),  # 28 → 14\n","\n","            # Block 5: 512 → 512\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),  # 14 → 7\n","        )\n","\n","        # After all pooling: 512 × 7 × 7 = 25,088\n","\n","        # Shared feature layer\n","        self.feature_fc = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(512 * 7 * 7, 1024),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # Classification head\n","        self.class_head = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","        # Attribute head\n","        self.attr_head = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, num_attributes)\n","        )\n","\n","    def forward(self, x):\n","        # Extract features from images ONLY\n","        x = self.features(x)\n","        x = torch.flatten(x, 1)\n","        features = self.feature_fc(x)\n","\n","        # Two predictions from same features\n","        class_logits = self.class_head(features)\n","        attr_logits = self.attr_head(features)\n","\n","        return class_logits, attr_logits"]},{"cell_type":"code","execution_count":31,"id":"a84e2a19","metadata":{"id":"a84e2a19","executionInfo":{"status":"ok","timestamp":1765290309144,"user_tz":-60,"elapsed":17,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}}},"outputs":[],"source":["from tqdm import tqdm\n","\n","def train_one_epoch_multitask(model, loader, optimizer, criterion_class,\n","                              criterion_attr, full_attributes, device, alpha=0.7):\n","    \"\"\"\n","    Train multi-task model for one epoch\n","\n","    Args:\n","        model: Your CNNMultiTask or ImprovedCNNMultiTask model\n","        loader: Training DataLoader\n","        optimizer: Optimizer (e.g., Adam)\n","        criterion_class: Loss for classification (e.g., CrossEntropyLoss)\n","        criterion_attr: Loss for attributes (e.g., MSELoss)\n","        full_attributes: numpy array of shape (200, 312) - full attributes matrix\n","        device: torch.device (cuda or cpu)\n","        alpha: weight for classification loss (1-alpha for attribute loss)\n","\n","    Returns:\n","        metrics: dict with 'total_loss', 'class_loss', 'attr_loss', 'accuracy'\n","    \"\"\"\n","    model.train()\n","    attributes_gpu = torch.FloatTensor(full_attributes).to(device)\n","\n","    # Initialize metrics tracking\n","    total_loss = 0.0\n","    total_class_loss = 0.0\n","    total_attr_loss = 0.0\n","    class_correct = 0\n","    total = 0\n","\n","    # Create progress bar\n","    progress_bar = tqdm(loader, desc=\"Training\", leave=False)\n","\n","    for batch in progress_bar:\n","        # Handle both 2-value and 3-value dataset returns\n","        if len(batch) == 2:\n","            images, labels = batch\n","        else:  # len(batch) == 3\n","            images, labels, _ = batch  # Ignore third value if present\n","        # Check device BEFORE movin\n","\n","        images = images.to(device, non_blocking=True)\n","        labels = labels.to(device, non_blocking=True)\n","\n","\n","\n","\n","        # Get ground truth attributes for this batch\n","        # labels is (batch_size,) with values [0, 199]\n","        # full_attributes[labels.cpu().numpy()] returns (batch_size, 312)\n","        batch_attrs = attributes_gpu[labels]\n","\n","        # Zero gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        class_logits, attr_logits = model(images)\n","\n","        # Calculate two losses\n","        loss_class = criterion_class(class_logits, labels)\n","        loss_attr = criterion_attr(attr_logits, batch_attrs)\n","\n","        # Combined loss with weighting\n","        loss = alpha * loss_class + (1 - alpha) * loss_attr\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Track metrics\n","        batch_size = images.size(0)\n","        total_loss += loss.item() * batch_size\n","        total_class_loss += loss_class.item() * batch_size\n","        total_attr_loss += loss_attr.item() * batch_size\n","\n","        # Calculate accuracy\n","        _, preds = class_logits.max(1)\n","        class_correct += (preds == labels).sum().item()\n","        total += batch_size\n","\n","        # Update progress bar with current metrics\n","        current_acc = class_correct / total\n","        progress_bar.set_postfix({\n","            'loss': f'{loss.item():.3f}',\n","            'acc': f'{current_acc:.3f}'\n","        })\n","\n","    # Calculate average metrics over entire epoch\n","    metrics = {\n","        'total_loss': total_loss / total,\n","        'class_loss': total_class_loss / total,\n","        'attr_loss': total_attr_loss / total,\n","        'accuracy': class_correct / total\n","    }\n","\n","    return metrics"]},{"cell_type":"code","execution_count":32,"id":"72696a4b","metadata":{"id":"72696a4b","executionInfo":{"status":"ok","timestamp":1765290312209,"user_tz":-60,"elapsed":18,"user":{"displayName":"Jason Jordan","userId":"00309143728606713150"}}},"outputs":[],"source":["def evaluate_multitask(model, loader, criterion_class, criterion_attr,\n","                       full_attributes, device, alpha=0.7):\n","    \"\"\"\n","    Evaluate multi-task model on validation/test set\n","\n","    Args:\n","        Same as train_one_epoch_multitask\n","\n","    Returns:\n","        metrics: dict with losses, accuracy, and predictions for analysis\n","    \"\"\"\n","    model.eval()\n","    attributes_gpu = torch.FloatTensor(full_attributes).to(device)\n","    # Initialize metrics tracking\n","    total_loss = 0.0\n","    total_class_loss = 0.0\n","    total_attr_loss = 0.0\n","    class_correct = 0\n","    total = 0\n","\n","    # Store predictions for confusion matrix later\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():  # No gradients needed for evaluation\n","        for batch in tqdm(loader, desc=\"Validation\", leave=False):\n","            # Handle both 2-value and 3-value dataset returns\n","            if len(batch) == 2:\n","                images, labels = batch\n","            else:\n","                images, labels, _ = batch\n","\n","\n","\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            # Check device AFTER moving\n","            # Get ground truth attributes\n","            batch_attrs = attributes_gpu[labels]\n","\n","            # Forward pass\n","            class_logits, attr_logits = model(images)\n","\n","            # Calculate losses\n","            loss_class = criterion_class(class_logits, labels)\n","            loss_attr = criterion_attr(attr_logits, batch_attrs)\n","            loss = alpha * loss_class + (1 - alpha) * loss_attr\n","\n","            # Track metrics\n","            batch_size = images.size(0)\n","            total_loss += loss.item() * batch_size\n","            total_class_loss += loss_class.item() * batch_size\n","            total_attr_loss += loss_attr.item() * batch_size\n","\n","            # Calculate accuracy\n","            _, preds = class_logits.max(1)\n","            class_correct += (preds == labels).sum().item()\n","            total += batch_size\n","\n","            # Store predictions and labels for later analysis\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Calculate average metrics\n","    metrics = {\n","        'total_loss': total_loss / total,\n","        'class_loss': total_class_loss / total,\n","        'attr_loss': total_attr_loss / total,\n","        'accuracy': class_correct / total,\n","        'predictions': all_preds,  # For confusion matrix\n","        'labels': all_labels        # For confusion matrix\n","    }\n","\n","    return metrics"]},{"cell_type":"code","execution_count":null,"id":"a2ce3afa","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"a2ce3afa","outputId":"e8b10cf4-d120-4104-d387-771fbdadc33e"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","TRAINING IMPROVED CNN FROM SCRATCH\n","======================================================================\n","Model parameters: 30,917,120\n","Learning rate: 5e-05\n","Epochs: 25\n","Alpha (class weight): 0.8\n","======================================================================\n","\n","\n","Epoch 1/25\n"]},{"output_type":"stream","name":"stderr","text":["Training:  46%|████▌     | 23/50 [04:14<04:01,  8.93s/it, loss=4.315, acc=0.008]"]}],"source":["\n","# Setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","attributes_full = np.load(glob_path + 'data/attributes.npy')\n","\n","# Use improved model\n","model = ImprovedCNNMultiTask(num_classes=200, num_attributes=312).to(device)\n","\n","# Loss functions\n","criterion_class = nn.CrossEntropyLoss()\n","criterion_attr = nn.MSELoss()\n","\n","# IMPORTANT: Lower learning rate for training from scratch\n","optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)  # Much lower!\n","\n","# Add learning rate scheduler\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n","\n","# Training config\n","num_epochs = 25  # More epochs for from-scratch training\n","alpha = 0.8  # Focus more on classification\n","best_val_acc = 0.0\n","\n","history = {\n","    'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [],\n","    'train_class_loss': [], 'train_attr_loss': [],\n","    'val_class_loss': [], 'val_attr_loss': []\n","}\n","\n","print(\"=\"*70)\n","print(f\"TRAINING IMPROVED CNN FROM SCRATCH\")\n","print(\"=\"*70)\n","print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n","print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n","print(f\"Epochs: {num_epochs}\")\n","print(f\"Alpha (class weight): {alpha}\")\n","print(\"=\"*70 + \"\\n\")\n","\n","for epoch in range(1, num_epochs + 1):\n","    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n","\n","    train_metrics = train_one_epoch_multitask(\n","        model, train_loader, optimizer, criterion_class,\n","        criterion_attr, attributes_full, device, alpha\n","    )\n","\n","    val_metrics = evaluate_multitask(\n","        model, val_loader, criterion_class, criterion_attr,\n","        attributes_full, device, alpha\n","    )\n","\n","    # Update learning rate\n","    scheduler.step()\n","\n","    # Store history\n","    history['train_loss'].append(train_metrics['total_loss'])\n","    history['train_acc'].append(train_metrics['accuracy'])\n","    history['val_loss'].append(val_metrics['total_loss'])\n","    history['val_acc'].append(val_metrics['accuracy'])\n","    history['train_class_loss'].append(train_metrics['class_loss'])\n","    history['train_attr_loss'].append(train_metrics['attr_loss'])\n","    history['val_class_loss'].append(val_metrics['class_loss'])\n","    history['val_attr_loss'].append(val_metrics['attr_loss'])\n","\n","    print(f\"Train: Loss={train_metrics['total_loss']:.4f}, Acc={train_metrics['accuracy']:.4f} ({train_metrics['accuracy']*100:.2f}%)\")\n","    print(f\"Val:   Loss={val_metrics['total_loss']:.4f}, Acc={val_metrics['accuracy']:.4f} ({val_metrics['accuracy']*100:.2f}%)\")\n","    print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n","\n","    if val_metrics['accuracy'] > best_val_acc:\n","        best_val_acc = val_metrics['accuracy']\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'val_acc': val_metrics['accuracy'],\n","            'history': history\n","        }, \"best_improved_cnn_multitask.pt\")\n","        print(f\"✅ New best! Val Acc: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n","\n","print(f\"\\n🎉 Training complete! Best Val Acc: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")"]},{"cell_type":"code","execution_count":null,"id":"82b0c01d","metadata":{"id":"82b0c01d"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"2d398f1b","metadata":{"id":"2d398f1b"},"outputs":[],"source":["CNN_model = SimpleCNN(num_classes=200).to(device)\n","\n","# Define loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr=3e-4)"]},{"cell_type":"markdown","id":"2d0f7d8b","metadata":{"id":"2d0f7d8b"},"source":["### Define a baseline CNN model (ResNet18)\n","\n","As a strong baseline, we use a pretrained `ResNet18` from `torchvision.models`:\n","- We load ImageNet-pretrained weights.\n","- We replace the final fully-connected layer so it outputs 200 logits (one per bird class).\n","- The rest of the network acts as a feature extractor.\n","\n","This gives a solid starting point for accuracy without heavy custom architecture work.\n"]},{"cell_type":"code","execution_count":null,"id":"6380dfb2","metadata":{"id":"6380dfb2"},"outputs":[],"source":["# Load pretrained ResNet18\n","weights = models.ResNet18_Weights.IMAGENET1K_V1\n","ResNet_model = models.resnet18(weights=weights)\n","\n","# Replace the final layer to match 200 classes\n","num_features = ResNet_model.fc.in_features\n","ResNet_model.fc = nn.Linear(num_features, 200)\n","\n","ResNet_model = ResNet_model.to(device)\n","\n","# Define loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","ResNet_optimizer = torch.optim.Adam(ResNet_model.parameters(), lr=1e-4)"]},{"cell_type":"markdown","id":"9b211d20","metadata":{"id":"9b211d20"},"source":["### Define training and validation loops\n","\n","Here we implement two functions:\n","\n","- `train_one_epoch`: runs one epoch over the training set, updates weights, and tracks loss and accuracy.\n","- `evaluate`: runs one full pass over the validation set without gradient updates, and reports loss and accuracy.\n","\n","These utilities keep the main training loop clean and readable, and allow easy reuse later."]},{"cell_type":"code","execution_count":null,"id":"b0a9c0f6","metadata":{"id":"b0a9c0f6"},"outputs":[],"source":["from tqdm import tqdm\n","\n","def train_one_epoch(model, loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for pixel_values, labels, attributes in tqdm(loader, desc=\"Train\", leave=False):\n","        pixel_values = pixel_values.to(device)\n","        labels = labels.to(device)\n","        attributes = attributes.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        logits = model(pixel_values, attributes)\n","        #outputs = model(imgs)\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * pixel_values.size(0)\n","        _, preds = logits.max(1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    avg_loss = total_loss / total\n","    accuracy = correct / total\n","    return avg_loss, accuracy\n","\n","\n","def evaluate(model, loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for pixel_values, labels, attributes in tqdm(loader, desc=\"Val\", leave=False):\n","            pixel_values = pixel_values.to(device)\n","            labels = labels.to(device)\n","            attributes = attributes.to(device)\n","\n","            logits = model(pixel_values, attributes)\n","            loss = criterion(logits, labels)\n","\n","            total_loss += loss.item() * pixel_values.size(0)\n","            _, preds = logits.max(1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","\n","    avg_loss = total_loss / total\n","    accuracy = correct / total\n","    return avg_loss, accuracy\n"]},{"cell_type":"markdown","id":"f3b9bf68","metadata":{"id":"f3b9bf68"},"source":["### Train the CNN baseline model and monitor accuracy\n","\n","We run the training for a few epochs.  \n","For each epoch, we log:\n","- Training loss and accuracy\n","- Validation loss and accuracy\n","\n","We also keep track of the best validation accuracy and save the model weights whenever a new best score is reached.  \n","This gives me a first baseline performance for the bird classification task.\n"]},{"cell_type":"code","execution_count":null,"id":"b45ef435","metadata":{"id":"b45ef435"},"outputs":[],"source":["attributes_full = np.load('data/attributes.npy')"]},{"cell_type":"code","execution_count":null,"id":"68ae3285","metadata":{"id":"68ae3285"},"outputs":[],"source":["\n","import matplotlib.pyplot as plt\n","\n","# ============================================\n","# TRAINING FUNCTIONS WITH DETAILED METRICS\n","# ============================================\n","\n","def train_one_epoch_multitask(model, loader, optimizer, criterion_class,\n","                              criterion_attr, full_attributes, device, alpha=0.7):\n","    \"\"\"Train for one epoch and return detailed metrics\"\"\"\n","    model.train()\n","\n","    # Metrics tracking\n","    total_loss = 0.0\n","    total_class_loss = 0.0\n","    total_attr_loss = 0.0\n","    class_correct = 0\n","    total = 0\n","\n","    progress_bar = tqdm(loader, desc=\"Training\", leave=False)\n","\n","    for batch in progress_bar:\n","        # Handle both 2-value and 3-value returns\n","        if len(batch) == 2:\n","            images, labels = batch\n","        else:  # len(batch) == 3\n","            images, labels, _ = batch\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Get ground truth attributes\n","        batch_attrs = full_attributes[labels.cpu().numpy()]\n","        batch_attrs = torch.FloatTensor(batch_attrs).to(device)\n","\n","        # Forward pass\n","        optimizer.zero_grad()\n","        class_logits, attr_logits = model(images)\n","\n","        # Calculate losses\n","        loss_class = criterion_class(class_logits, labels)\n","        loss_attr = criterion_attr(attr_logits, batch_attrs)\n","        loss = alpha * loss_class + (1 - alpha) * loss_attr\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Track metrics\n","        batch_size = images.size(0)\n","        total_loss += loss.item() * batch_size\n","        total_class_loss += loss_class.item() * batch_size\n","        total_attr_loss += loss_attr.item() * batch_size\n","\n","        _, preds = class_logits.max(1)\n","        class_correct += (preds == labels).sum().item()\n","        total += batch_size\n","\n","        # Update progress bar\n","        current_acc = class_correct / total\n","        progress_bar.set_postfix({\n","            'loss': f'{loss.item():.3f}',\n","            'acc': f'{current_acc:.3f}'\n","        })\n","\n","    # Calculate average metrics\n","    metrics = {\n","        'total_loss': total_loss / total,\n","        'class_loss': total_class_loss / total,\n","        'attr_loss': total_attr_loss / total,\n","        'accuracy': class_correct / total\n","    }\n","\n","    return metrics\n","\n","\n","def evaluate_multitask(model, loader, criterion_class, criterion_attr,\n","                       full_attributes, device, alpha=0.7):\n","    \"\"\"Evaluate model and return detailed metrics\"\"\"\n","    model.eval()\n","\n","    # Metrics tracking\n","    total_loss = 0.0\n","    total_class_loss = 0.0\n","    total_attr_loss = 0.0\n","    class_correct = 0\n","    total = 0\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(loader, desc=\"Training\", leave=False)\n","\n","        for batch in progress_bar:\n","            # Handle both 2-value and 3-value returns\n","            if len(batch) == 2:\n","                images, labels = batch\n","            else:  # len(batch) == 3\n","                images, labels, _ = batch\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            # Get ground truth attributes\n","            batch_attrs = full_attributes[labels.cpu().numpy()]\n","            batch_attrs = torch.FloatTensor(batch_attrs).to(device)\n","\n","            # Forward pass\n","            class_logits, attr_logits = model(images)\n","\n","            # Calculate losses\n","            loss_class = criterion_class(class_logits, labels)\n","            loss_attr = criterion_attr(attr_logits, batch_attrs)\n","            loss = alpha * loss_class + (1 - alpha) * loss_attr\n","\n","            # Track metrics\n","            batch_size = images.size(0)\n","            total_loss += loss.item() * batch_size\n","            total_class_loss += loss_class.item() * batch_size\n","            total_attr_loss += loss_attr.item() * batch_size\n","\n","            _, preds = class_logits.max(1)\n","            class_correct += (preds == labels).sum().item()\n","            total += batch_size\n","\n","            # Store for confusion matrix later\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Calculate average metrics\n","    metrics = {\n","        'total_loss': total_loss / total,\n","        'class_loss': total_class_loss / total,\n","        'attr_loss': total_attr_loss / total,\n","        'accuracy': class_correct / total,\n","        'predictions': all_preds,\n","        'labels': all_labels\n","    }\n","\n","    return metrics\n","\n","\n","# ============================================\n","# TRAINING LOOP WITH FULL METRICS DISPLAY\n","# ============================================\n","\n"]},{"cell_type":"code","execution_count":null,"id":"74eee8ab","metadata":{"scrolled":true,"id":"74eee8ab"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\\n\")\n","\n","# Load attributes\n","attributes_full = np.load('data/attributes.npy')\n","\n","# Initialize model\n","model = CNNMultiTask(num_classes=200, num_attributes=312).to(device)\n","\n","# Loss functions\n","criterion_class = nn.CrossEntropyLoss()\n","criterion_attr = nn.MSELoss()\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Training configuration\n","num_epochs = 15\n","alpha = 0.7  # Weight for classification loss\n","best_val_acc = 0.0\n","\n","# Metrics history for plotting\n","history = {\n","    'train_loss': [],\n","    'train_acc': [],\n","    'val_loss': [],\n","    'val_acc': [],\n","    'train_class_loss': [],\n","    'train_attr_loss': [],\n","    'val_class_loss': [],\n","    'val_attr_loss': []\n","}\n","\n","print(\"=\"*70)\n","print(f\"{'TRAINING MULTI-TASK CNN':^70}\")\n","print(\"=\"*70)\n","print(f\"Model: CNNMultiTask\")\n","print(f\"Num classes: 200\")\n","print(f\"Num attributes: 312\")\n","print(f\"Loss weight (alpha): {alpha}\")\n","print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n","print(f\"Batch size: {train_loader.batch_size}\")\n","print(f\"Train samples: {len(train_dataset)}\")\n","print(f\"Val samples: {len(val_dataset)}\")\n","print(\"=\"*70 + \"\\n\")\n","\n","# Training loop\n","for epoch in range(1, num_epochs + 1):\n","    print(f\"\\n{'='*70}\")\n","    print(f\"Epoch {epoch}/{num_epochs}\")\n","    print(f\"{'='*70}\")\n","\n","    # Train\n","    train_metrics = train_one_epoch_multitask(\n","        model, train_loader, optimizer, criterion_class,\n","        criterion_attr, attributes_full, device, alpha\n","    )\n","\n","    # Validate\n","    val_metrics = evaluate_multitask(\n","        model, val_loader, criterion_class, criterion_attr,\n","        attributes_full, device, alpha\n","    )\n","\n","    # Store history\n","    history['train_loss'].append(train_metrics['total_loss'])\n","    history['train_acc'].append(train_metrics['accuracy'])\n","    history['val_loss'].append(val_metrics['total_loss'])\n","    history['val_acc'].append(val_metrics['accuracy'])\n","    history['train_class_loss'].append(train_metrics['class_loss'])\n","    history['train_attr_loss'].append(train_metrics['attr_loss'])\n","    history['val_class_loss'].append(val_metrics['class_loss'])\n","    history['val_attr_loss'].append(val_metrics['attr_loss'])\n","\n","    # Print metrics\n","    print(f\"\\n📊 TRAINING METRICS:\")\n","    print(f\"   Total Loss:    {train_metrics['total_loss']:.4f}\")\n","    print(f\"   Class Loss:    {train_metrics['class_loss']:.4f}\")\n","    print(f\"   Attr Loss:     {train_metrics['attr_loss']:.4f}\")\n","    print(f\"   Accuracy:      {train_metrics['accuracy']:.4f} ({train_metrics['accuracy']*100:.2f}%)\")\n","\n","    print(f\"\\n📊 VALIDATION METRICS:\")\n","    print(f\"   Total Loss:    {val_metrics['total_loss']:.4f}\")\n","    print(f\"   Class Loss:    {val_metrics['class_loss']:.4f}\")\n","    print(f\"   Attr Loss:     {val_metrics['attr_loss']:.4f}\")\n","    print(f\"   Accuracy:      {val_metrics['accuracy']:.4f} ({val_metrics['accuracy']*100:.2f}%)\")\n","\n","    # Check if best model\n","    if val_metrics['accuracy'] > best_val_acc:\n","        best_val_acc = val_metrics['accuracy']\n","\n","        # Save checkpoint\n","        checkpoint = {\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'val_acc': val_metrics['accuracy'],\n","            'val_loss': val_metrics['total_loss'],\n","            'train_acc': train_metrics['accuracy'],\n","            'history': history\n","        }\n","        torch.save(checkpoint, \"best_multitask_cnn.pt\")\n","\n","        print(f\"\\n✅ NEW BEST MODEL SAVED!\")\n","        print(f\"   Validation Accuracy: {val_metrics['accuracy']:.4f} ({val_metrics['accuracy']*100:.2f}%)\")\n","        print(f\"   Improvement: +{(val_metrics['accuracy'] - (history['val_acc'][-2] if len(history['val_acc']) > 1 else 0))*100:.2f}%\")\n","    else:\n","        print(f\"\\n   Best Val Acc: {best_val_acc:.4f} (Epoch {history['val_acc'].index(best_val_acc) + 1})\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(f\"{'TRAINING COMPLETE':^70}\")\n","print(\"=\"*70)\n","print(f\"\\n🎉 Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n","print(f\"📁 Model saved to: best_multitask_cnn.pt\\n\")"]},{"cell_type":"code","execution_count":null,"id":"ead9c3b2","metadata":{"id":"ead9c3b2"},"outputs":[],"source":["num_epochs = 5\n","best_val_acc = 0.0\n","\n","model = CNNMultiTask(num_classes=200, num_attributes=312).to(device)\n","\n","criterion_class = nn.CrossEntropyLoss()\n","criterion_attr = nn.BCEWithLogitsLoss()  # Or MSELoss if attributes are continuous\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","\n","    for images, labels, attributes in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        # Get ground truth attributes for these classes\n","        true_attributes = torch.FloatTensor(attributes_full[labels.cpu().numpy()]).to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        class_logits, attr_logits = model(images)\n","\n","        # Two losses\n","        loss_class = criterion_class(class_logits, labels)\n","        loss_attr = criterion_attr(attr_logits, true_attributes)\n","\n","        # Combined loss (you can weight these)\n","        loss = 0.7 * loss_class + 0.3 * loss_attr\n","\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"code","execution_count":null,"id":"f38f4fba-d4ed-4529-9456-0816845b64a1","metadata":{"id":"f38f4fba-d4ed-4529-9456-0816845b64a1"},"outputs":[],"source":["num_epochs = 5\n","best_val_acc = 0.0\n","\n","for epoch in range(1, num_epochs + 1):\n","    print(f\"Epoch {epoch}/{num_epochs}\")\n","\n","    train_loss, train_acc = train_one_epoch(CNN_model, train_loader, CNN_optimizer, criterion, device)\n","    val_loss, val_acc = evaluate(CNN_model, val_loader, criterion, device)\n","\n","    print(f\"  Train  | loss: {train_loss:.4f}, acc: {train_acc:.4f}\")\n","    print(f\"  Val    | loss: {val_loss:.4f}, acc: {val_acc:.4f}\")\n","\n","    # Save best ResNet_model\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        torch.save(CNN_model.state_dict(), \"best_CNN_baseline.pt\")\n","        print(f\"New best model saved with val_acc = {best_val_acc:.4f}\")\n"]},{"cell_type":"markdown","id":"d5099460-18ba-447c-931e-4e57afe3c980","metadata":{"id":"d5099460-18ba-447c-931e-4e57afe3c980"},"source":["### Train the ResNet18 baseline model and monitor accuracy\n"]},{"cell_type":"code","execution_count":null,"id":"2b8486ab","metadata":{"id":"2b8486ab"},"outputs":[],"source":["num_epochs = 5\n","best_val_acc = 0.0\n","\n","for epoch in range(1, num_epochs + 1):\n","    print(f\"Epoch {epoch}/{num_epochs}\")\n","\n","    train_loss, train_acc = train_one_epoch(ResNet_model, train_loader, ResNet_optimizer, criterion, device)\n","    val_loss, val_acc = evaluate(ResNet_model, val_loader, criterion, device)\n","\n","    print(f\"  Train  | loss: {train_loss:.4f}, acc: {train_acc:.4f}\")\n","    print(f\"  Val    | loss: {val_loss:.4f}, acc: {val_acc:.4f}\")\n","\n","    # Save best ResNet_model\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        torch.save(ResNet_model.state_dict(), \"best_resnet18_baseline.pt\")\n","        print(f\"New best model saved with val_acc = {best_val_acc:.4f}\")\n"]},{"cell_type":"markdown","id":"31fb1136-7208-4f13-b776-37563eff0ce9","metadata":{"id":"31fb1136-7208-4f13-b776-37563eff0ce9"},"source":["### Re-load the ResNet18 model (No need to run)"]},{"cell_type":"code","execution_count":null,"id":"0ed50a0e-8d06-4ec3-9c6a-3e3ad0a4c31e","metadata":{"id":"0ed50a0e-8d06-4ec3-9c6a-3e3ad0a4c31e"},"outputs":[],"source":["ResNet_model = models.resnet18(weights=None)  # initialize architecture\n","num_features = ResNet_model.fc.in_features\n","ResNet_model.fc = nn.Linear(num_features, 200)\n","\n","ResNet_model.load_state_dict(torch.load(\"best_resnet18_baseline.pt\", map_location=device))\n","ResNet_model = ResNet_model.to(device)\n","ResNet_model.eval()"]},{"cell_type":"markdown","id":"2b2fb120-0515-47e8-b62f-ed38036f222d","metadata":{"id":"2b2fb120-0515-47e8-b62f-ed38036f222d"},"source":["## Load `Falconsai/nsfw_image_detection` and adapt it for 200 bird classes\n","\n","The `Falconsai/nsfw_image_detection` model is a ViT-based image classifier originally trained for 2 classes\n","(`normal` vs `nsfw`). I reuse the pretrained backbone and:\n","\n","1. Load the model and its image processor from Hugging Face.\n","2. Replace the final classification layer (`classifier`) so that it outputs 200 logits (one per bird class).\n","3. Update the config metadata (`num_labels`, `id2label`, `label2id`) for consistency.\n","\n","This gives me a strong transformer-based model specialized for my 200 bird classes.\n"]},{"cell_type":"code","execution_count":null,"id":"3634a60f-09da-4418-aaea-91fce90bf6e2","metadata":{"id":"3634a60f-09da-4418-aaea-91fce90bf6e2"},"outputs":[],"source":["## !pip install transformers\n","\n","from transformers import AutoModelForImageClassification, AutoImageProcessor\n","import torch\n","import torch.nn as nn\n"]},{"cell_type":"code","execution_count":null,"id":"8e3a1371-6e68-4f9b-9bdd-7eae72e83366","metadata":{"id":"8e3a1371-6e68-4f9b-9bdd-7eae72e83366"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","model_name = \"Falconsai/nsfw_image_detection\"\n","\n","# Image processor: handles resize, normalize, etc. for ViT\n","processor = AutoImageProcessor.from_pretrained(model_name)\n","\n","# Load ViT-based image classification model\n","vit_model = AutoModelForImageClassification.from_pretrained(model_name)\n","\n","print(\"Original num_labels:\", vit_model.config.num_labels)\n","\n","# Replace classifier head to output 200 classes\n","num_features = vit_model.classifier.in_features\n","vit_model.classifier = nn.Linear(num_features, 200)\n","\n","# Update config info\n","vit_model.config.num_labels = 200\n","vit_model.num_labels = 200\n","vit_model.config.id2label = {i: f\"class_{i+1}\" for i in range(200)}\n","vit_model.config.label2id = {v: k for k, v in vit_model.config.id2label.items()}\n","\n","vit_model = vit_model.to(device)\n","print(\"Adapted num_labels:\", vit_model.config.num_labels)\n"]},{"cell_type":"markdown","id":"8c675c9a-2cb9-40da-af1a-99009661fef8","metadata":{"id":"8c675c9a-2cb9-40da-af1a-99009661fef8"},"source":["### Create a Dataset that uses the ViT image processor\n","\n","For the ViT model, I no longer use the `torchvision` transforms.\n","Instead, I use the Hugging Face `AutoImageProcessor`, which:\n","\n","- Resizes the image to the correct resolution (224×224 for ViT)\n","- Converts it to a tensor\n","- Applies the exact normalization used during pretraining\n","\n","I define a `BirdsDatasetViT` class that:\n","- Takes the same `train_df` / `val_df` as before (with `image_path` and `label`)\n","- Loads each image with PIL\n","- Runs the image through the processor to get `pixel_values`\n","- Returns `(pixel_values, label)` where labels are 0–199\n"]},{"cell_type":"code","execution_count":null,"id":"2b66c1ed-18bf-4706-80d6-9da57494c66f","metadata":{"id":"2b66c1ed-18bf-4706-80d6-9da57494c66f"},"outputs":[],"source":["from torch.utils.data import Dataset\n","from PIL import Image\n","import os\n","\n","class BirdsDatasetViT(Dataset):\n","    def __init__(self, df, processor, base_dir=\".\", label_col=\"label\", path_col=\"image_path\"):\n","        self.df = df.reset_index(drop=True)\n","        self.processor = processor\n","        self.base_dir = base_dir\n","        self.label_col = label_col\n","        self.path_col = path_col\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        raw_path = str(row[self.path_col])\n","        # Fix leading \"/\" → make it relative\n","        rel_path = raw_path.lstrip(\"/\")\n","        img_path = os.path.join(self.base_dir, rel_path)\n","\n","        if not os.path.exists(img_path):\n","            raise FileNotFoundError(f\"Image not found: {img_path}\")\n","\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        # Use HF processor to get ViT-ready pixel_values\n","        inputs = self.processor(images=img, return_tensors=\"pt\")\n","        pixel_values = inputs[\"pixel_values\"].squeeze(0)  # (3, H, W)\n","\n","        label = int(row[self.label_col]) - 1  # 1–200 → 0–199\n","\n","        return pixel_values, label\n"]},{"cell_type":"markdown","id":"b5904d4b-7328-4942-b867-3347891a1743","metadata":{"id":"b5904d4b-7328-4942-b867-3347891a1743"},"source":["### Create DataLoaders for the ViT-based model\n","\n","Now I wrap the `BirdsDatasetViT` in PyTorch DataLoaders.\n","On macOS, using `num_workers=0` avoids multiprocess issues while debugging.\n","These loaders will feed ViT-ready `pixel_values` and labels into the training loop.\n"]},{"cell_type":"code","execution_count":null,"id":"35b79aa4-e444-4dd3-aebb-9c854264e538","metadata":{"id":"35b79aa4-e444-4dd3-aebb-9c854264e538"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","# Adjust this path: the folder that contains `train_images/`\n","# If your notebook is already in the project root, \".\" is fine.\n","base_dir = \".\"\n","\n","batch_size = 32\n","\n","train_dataset_vit = BirdsDatasetViT(train_images, processor=processor, base_dir=base_dir)\n","val_dataset_vit   = BirdsDatasetViT(val_images,   processor=processor, base_dir=base_dir)\n","\n","train_loader_vit = DataLoader(\n","    train_dataset_vit,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader_vit = DataLoader(\n","    val_dataset_vit,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=0\n",")"]},{"cell_type":"markdown","id":"c539b415-37fc-40d4-a848-32f0686d5d01","metadata":{"id":"c539b415-37fc-40d4-a848-32f0686d5d01"},"source":["### Training and validation loops for the ViT model\n","\n","The training logic is the same as before, but the forward pass changes slightly:\n","\n","- For ResNet: `outputs = model(images)`\n","- For ViT (Hugging Face): `outputs = vit_model(pixel_values=images)`\n","\n","From `outputs`, I use `outputs.logits` and compute cross-entropy loss as usual.\n","The rest of the loop (accuracy computation, backprop, logging) is unchanged.\n"]},{"cell_type":"code","execution_count":null,"id":"61f36835-85bb-4713-963a-8e5908156138","metadata":{"id":"61f36835-85bb-4713-963a-8e5908156138"},"outputs":[],"source":["from tqdm import tqdm\n","\n","def train_one_epoch_vit(model, loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for pixel_values, labels in tqdm(loader, desc=\"Train (ViT)\", leave=False):\n","        pixel_values = pixel_values.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(pixel_values=pixel_values)\n","        logits = outputs.logits\n","\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * pixel_values.size(0)\n","        _, preds = logits.max(1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    avg_loss = total_loss / total\n","    accuracy = correct / total\n","    return avg_loss, accuracy\n","\n","\n","def evaluate_vit(model, loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for pixel_values, labels in tqdm(loader, desc=\"Val (ViT)\", leave=False):\n","            pixel_values = pixel_values.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(pixel_values=pixel_values)\n","            logits = outputs.logits\n","\n","            loss = criterion(logits, labels)\n","\n","            total_loss += loss.item() * pixel_values.size(0)\n","            _, preds = logits.max(1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","\n","    avg_loss = total_loss / total\n","    accuracy = correct / total\n","    return avg_loss, accuracy\n"]},{"cell_type":"markdown","id":"d91a6a78-6c71-4cd9-907d-bcff31a08bc7","metadata":{"id":"d91a6a78-6c71-4cd9-907d-bcff31a08bc7"},"source":["### Fine-tune the Falconsai ViT as a \"ceiling\" model\n","\n","Now I fine-tune the adapted ViT model on the bird dataset.\n","This model:\n","\n","- Starts from a powerful Vision Transformer backbone\n","- Has a new classification head for 200 bird classes\n","- Is expected to perform at least as well as ResNet18, and possibly better,\n","  giving me an approximate performance \"ceiling\" for this assignment.\n","\n","I reuse the same training hyperparameters as a starting point and monitor train/validation accuracy.\n"]},{"cell_type":"code","execution_count":null,"id":"4f17d712-7302-4431-8a55-a0dc5d23ce94","metadata":{"id":"4f17d712-7302-4431-8a55-a0dc5d23ce94"},"outputs":[],"source":["criterion_vit = nn.CrossEntropyLoss()\n","optimizer_vit = torch.optim.Adam(vit_model.parameters(), lr=1e-4)\n","\n","num_epochs_vit = 5\n","best_val_acc_vit = 0.0\n","\n","for epoch in range(1, num_epochs_vit + 1):\n","    print(f\"Epoch {epoch}/{num_epochs_vit} (ViT)\")\n","\n","    train_loss, train_acc = train_one_epoch_vit(vit_model, train_loader_vit, optimizer_vit, criterion_vit, device)\n","    val_loss, val_acc = evaluate_vit(vit_model, val_loader_vit, criterion_vit, device)\n","\n","    print(f\"  Train (ViT) | loss: {train_loss:.4f}, acc: {train_acc:.4f}\")\n","    print(f\"  Val   (ViT) | loss: {val_loss:.4f}, acc: {val_acc:.4f}\")\n","\n","    if val_acc > best_val_acc_vit:\n","        best_val_acc_vit = val_acc\n","        torch.save(vit_model.state_dict(), \"vit_nsfw_birds_state_dict.pt\")\n","        print(f\" New best ViT model saved with val_acc = {best_val_acc_vit:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"43e10e69","metadata":{"id":"43e10e69"},"outputs":[],"source":["class BirdsTestDataset(Dataset):\n","    \"\"\"\n","    Dataset for test images (no labels, no attributes needed)\n","    \"\"\"\n","    def __init__(self, df, transform=None, processor=None, path_col=\"image_path\"):\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transform\n","        self.processor = processor\n","        self.path_col = path_col\n","\n","        if transform is None and processor is None:\n","            raise ValueError(\"Must provide either transform or processor\")\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        img_path = row[self.path_col]\n","\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        # Use processor OR transform\n","        if self.processor is not None:\n","            inputs = self.processor(images=img, return_tensors=\"pt\")\n","            pixel_values = inputs[\"pixel_values\"].squeeze(0)\n","        else:\n","            pixel_values = self.transform(img)\n","\n","        return pixel_values"]},{"cell_type":"code","execution_count":null,"id":"5b4ee891","metadata":{"id":"5b4ee891"},"outputs":[],"source":["test_df = pd.read_csv(\"data/test_images_sample.csv\")  # Should have columns: 'id', 'image_path'\n","test_path = pd.read_csv(\"data/test_images_path.csv\")\n","test_path"]},{"cell_type":"code","execution_count":null,"id":"6da33a30","metadata":{"id":"6da33a30"},"outputs":[],"source":["\n","import os\n","# Fix paths (adjust based on your file structure)\n","test_path[\"image_path\"] = test_path[\"image_path\"].apply(\n","    lambda x: os.path.join(\"data\", x.lstrip(\"/\"))\n",")\n","\n","# Verify first path exists\n","print(f\"First test image: {test_path.iloc[0]['image_path']}\")\n","print(f\"Exists? {os.path.exists(test_path.iloc[0]['image_path'])}\")\n","\n","print(f\"\\nTest set size: {len(test_path)}\")\n","test_path.head()"]},{"cell_type":"code","execution_count":null,"id":"f389b767","metadata":{"id":"f389b767"},"outputs":[],"source":["def generate_submission_cnn_with_attrs(model, test_df, transform, device,\n","                                       output_file=\"submission.csv\"):\n","    \"\"\"\n","    Generate Kaggle submission for CNN model that uses attributes\n","\n","    Args:\n","        model: Your CNNWithAttributes model\n","        test_df: DataFrame with test images\n","        attributes: (200, 312) numpy array\n","        transform: torchvision transforms\n","        device: torch.device\n","        output_file: name of output CSV file\n","    \"\"\"\n","    # Create test dataset and loader\n","    test_dataset = BirdsTestDataset(test_df, transform=transform)\n","    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","    # Load best model checkpoint\n","    model.to(device)\n","    model.eval()\n","\n","    all_predictions = []\n","\n","    print(\"Generating predictions...\")\n","    with torch.no_grad():\n","        for pixel_values in tqdm(test_loader):\n","            pixel_values = pixel_values.to(device)\n","            batch_size = pixel_values.size(0)\n","\n","            # For test images, we don't know the true class, so we need to\n","            # try predicting WITHOUT attributes, OR use a dummy approach\n","\n","            # OPTION 1: Predict without attributes (if model can handle it)\n","            # This won't work with CNNWithAttributes as-is\n","\n","            # OPTION 2: Use all-zeros attributes (dummy)\n","            dummy_attrs = torch.zeros(batch_size, 312).to(device)\n","            logits = model(pixel_values, dummy_attrs)\n","\n","            # Get predicted classes\n","            preds = torch.argmax(logits, dim=1)\n","\n","            # Convert 0-indexed to 1-indexed (1-200)\n","            preds = preds.cpu().numpy() + 1\n","            all_predictions.extend(preds)\n","\n","    # Create submission DataFrame\n","    submission_df = pd.DataFrame({\n","        'id': test_df['id'],\n","        'label': all_predictions\n","    })\n","\n","    # Save to CSV\n","    submission_df.to_csv(output_file, index=False)\n","    print(f\"✅ Saved predictions to {output_file}\")\n","\n","    return submission_df"]},{"cell_type":"code","execution_count":null,"id":"8c1f8181","metadata":{"id":"8c1f8181"},"outputs":[],"source":["cnn_model = SimpleCNN(num_classes=200).to(device)\n","\n","submission_cnn = generate_submission_cnn_with_attrs(\n","    model=cnn_model,\n","    test_df=test_path,\n","    transform=val_transform,\n","    device=device,\n","    #checkpoint_path=\"best_CNN_baseline.pt\",\n","    output_file=\"submission_cnn_baseline.csv\"\n",")"]},{"cell_type":"code","execution_count":null,"id":"4d8f0263","metadata":{"id":"4d8f0263"},"outputs":[],"source":["predictions_final = pd.read_csv('submission_cnn_baseline.csv')"]},{"cell_type":"code","execution_count":null,"id":"59296e04","metadata":{"id":"59296e04"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ce5f341b","metadata":{"id":"ce5f341b"},"outputs":[],"source":["predictions_final"]},{"cell_type":"code","execution_count":null,"id":"29b8f7b4","metadata":{"id":"29b8f7b4"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}